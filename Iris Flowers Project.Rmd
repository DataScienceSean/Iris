---
title: "Iris Flower Data Clean-up"
author: "Sean F. Larsen"
date: "April 2, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(ggplot2)
library(rpart)
library(rpart.plot)
library(caret)
```

## Ronald Fisher's Iris Flower Dataset
![](https://cdn-images-1.medium.com/max/1600/1*7bnLKsChXq94QjtAiRn40w.png)
British Statistician Ronald Fisher introduced the Iris Flower in 1936.  Fisher published a paper that described the use of multiple measurements in taxonomic problems as an example of linear discriminant analysis.

![**RONALD FISHER 1913**](https://upload.wikimedia.org/wikipedia/commons/a/aa/Youngronaldfisher2.JPG)

The Iris data is a favorite example when writing about R accessors, Data Exporting, Data importing, and for different visualization techniques.  Data Scientists and machine learning programmers often us this data set as an example and to test and train their first models.

## The Dataset
The Iris data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris Setosa, Versicolor, and Virginica. The data set has 150 cases (rows) and 5 variables (columns) named Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, and Species.

I acquired the dataset online in a CSV format. Using the gather function tidyr, I cleaned up the data and created a dataframe iris_df. The new data frame has 600 rows and four columns.

```{r include=FALSE}
# Read the file and create an iris data set in R.
iris <- read.csv(file="iris.csv")

# Using tydyr to gather and separet the values.
iris_tidy <- iris %>% gather(key, Value, -Species) %>% separate(key, c("Part", "Measure"), "\\.")

# Creating a datafram
iris_df <- data.frame(iris_tidy)

```

Here is a Scatter Plot of the Sepal and Pedal features of the three Species.

```{r echo=FALSE}
# Plot the dataframe
ggplot(iris_df, aes(x = Measure, y = Value, col = Part)) +
  geom_jitter() +
  facet_grid(. ~ Species)
```

## The Decision Tree and Machine Learning
The Iris data set is a complete data set for demonstrating the decision tree machine learning algorithm. A decision tree is a tool that uses a tree-like model set of decisions that lead the user to the answer. Decision trees are commonly used in operations research, specifically in decision analysis, to help identify a strategy most likely to reach a goal.

Decision trees have multiple benefits.  They are easy to understand and interpret, they work with categorical and numerical data, they require little data processing, and feature selection is automatic. They are not susceptible to outliers and can capture nonlinear relationships. The downside of decision trees is they are prone to overfitting, and with large complex data sets can be inaccurate.

## Training Test Split
Here we are taking the iris_df data frame and splitting it into two new files, iris_training to train the model and iris_test to test the model. The training data contains 80% of the original data, and the test model contains 20%. This model will determine which species of iris a flower is so that the test data won't contain the species column. This model uses the information index in the split criterion and the Complexity Parameter to measure to control tree growth. I set the CP to zero for the first run.

Once I've trained and tested the model with the data, the decision tree is plotted and then evaluated with a confusion matrix to display the results.

```{r echo=FALSE}
# The Split
n <- nrow(iris_df)
n_train <- round(.80 * n)

#set the Seed
set.seed(123)

# Create a vector of indicise which is an 80% random sample
train_indicise <- sample(1:n, n_train)

# subset the data frame into the training set
iris_train <- iris_df[train_indicise, ]

# Exclude the training indicise to create the test set
iris_test <- iris_df[-train_indicise, ]

# Train the model to predict "Species"
iris_model <- rpart(formula = Species ~.,
                    data = iris_train,
                    method = "class",
                    control = rpart.control(cp = 0),
                    parms = list(split = "information"))

# The Prediction
iris_pred <- predict(object = iris_model,
                     newdata = iris_test,
                     type = "class")

# Plotting the tree
prp(iris_model, extra = 1, faclen=0,  nn = T,
    box.col=c("green", "red"))

# The Confusion Matrix
confusionMatrix(data = iris_pred,
                reference = iris_test$Species)
```
## First Run of the Model
If you look at the scatter plot, you see that the Iris Species Versicolor and Virginica are very similar. At first pass, this model is 75% accurate. For the Setosa species, it was the most accurate.

To better evaluate this model, the next step would be to look at CP. The xerror is the optimal value for CP. The model calculated accuracy and stored in variable base_accuracy.

```{r echo=FALSE}

printcp(iris_model)
plotcp(iris_model)

```

## Pruning the Tree
One problem to avoid is overfitting the model.  You prevent overfitting with pruning.  There are two types of pruning. Pre-pruning and Post-pruning.  Pre-pruning is a stop criterion built into the rpart package. I am setting the CP to 0, the max depth to 8, and the min split to 50.

```{r echo=FALSE}
# Grow a tree with minsplit of 100 and max depth of 8
iris_model_preprun <- rpart(formula = Species ~.,
                    data = iris_train,
                    method = "class",
                    control = rpart.control(cp = 0),
                    maxdepth = 8,
                    minsplit = 50)

# Compute the accuracy of the pruned tree
iris_pred_preprun <- predict(object = iris_model_preprun,
                     newdata = iris_test,
                     type = "class")

# Plotting the tree
prp(iris_model_preprun, extra = 1, faclen=0,  nn = T,
    box.col=c("green", "red"))

# The Confusion Matrix
confusionMatrix(data = iris_pred_preprun,
                reference = iris_test$Species)
printcp(iris_model_preprun)
plotcp(iris_model_preprun)

```

With the Pre-Pruning results, we can see that the overall accuracy didn't improve. For the most part each speicies results remained the same.

## Post-Pruning

With post-pruning, you allow the tree to grow to its full extend and only adjust the CP. I set the CP to 0.039

```{r echo=FALSE}

# Grow a tree with minsplit of 100 and max depth of 8
iris_model_postprun <- rpart(formula = Species ~.,
                    data = iris_train,
                    method = "class",
                    control = rpart.control(cp = 0.039),
                    maxdepth = 9,
                    minsplit = 50)

# Compute the accuracy of the pruned tree
iris_pred_postprun <- predict(object = iris_model_postprun,
                     newdata = iris_test,
                     type = "class")

# Plotting the tree
prp(iris_model_postprun, extra = 1, faclen=0,  nn = T,
    box.col=c("green", "red"))

# The Confusion Matrix
confusionMatrix(data = iris_pred_postprun,
                reference = iris_test$Species)
printcp(iris_model_postprun)
plotcp(iris_model_postprun)
```

# Conclusion
The accuracy of a model is improved when you prune the tree. Depending on the model, both pre-pruning and post-pruning may be needed to prevent overfitting, underfitting, and to fine-tune the model.